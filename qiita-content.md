# ã‚ã‚ŠãŒã¡ã ã‘ã©ã€Pythonã§ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆä½œã£ã¦ã¿ãŸ

Pythonãƒ»æ©Ÿæ¢°å­¦ç¿’ã®åˆå¿ƒè€…ã§ã™ãŒã€éŠã³ã§ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆä½œã£ã¦ã¿ã¾ã—ãŸã€‚
ä¼¼ãŸã‚ˆã†ãªè¨˜äº‹ã¯ã‚ˆãã‚ã‚‹æ°—ãŒã—ã¾ã™ãŒã€qiitaã«æŠ•ç¨¿ã—ã¦ã¿ãŸã‹ã£ãŸã®ã¨ä½•ã‹æ„è¦‹ãŒã‚‚ã‚‰ãˆãŸã‚‰å¬‰ã—ã„ã®ã§æ‰‹é †ã‚’ã¾ã¨ã‚ã¾ã—ãŸã€‚

## ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®æ¦‚è¦

è¨€èªã¯Pythonã§ã€MeCabã¨Word2Vecã‚’åˆ©ç”¨ã—ã€å…¥åŠ›ã•ã‚ŒãŸå˜èªã«å¯¾ã—ã¦ãƒãƒ«ã‚³ãƒ•é€£é–ã§è‡ªå‹•ä½œæ–‡ã‚’ã•ã›ã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§ã™ã€‚
ã€€â€»å…¥åŠ›ã•ã‚ŒãŸ**æ–‡ç« ã§ã¯ãªãå˜èª1èª**

ã¡ãªã¿ã«ç§ã¯è‡ªåˆ†ã®æ—¥è¨˜ï¼”å¹´åˆ†ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å­¦ç¿’ã•ã›ã¾ã—ãŸã€‚  
æ­£ç›´ãªã¨ã“ã‚ã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã¨ã—ã¦ã®ã‚¯ã‚ªãƒªãƒ†ã‚£ãƒ¼ãŒä½ã™ãã¦ä¼šè©±ã—ã¦ã‚‹æ„Ÿã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãŒã€ã€Œæ˜”ã“ã†ã„ã†è¨€è‘‰ã‚ˆãä½¿ã£ã¦ãŸãª~ã€ã¨ã„ã†æ¥½ã—ã¿ã¯ã‚ã‚Šã¾ã—ãŸã€‚  

## å‹•ã‹ã—ãŸç’°å¢ƒ

- Windows 10ï¼ˆãƒ›ã‚¹ãƒˆOSï¼‰
  - VM VirtualBox -> Docker Toolbox
    - Ubuntu 16.04
      - Jupyter Notebook
      - Python 3
      - MeCab
      - Word2Vec
  - utf-8ã‹shift_jis ã®txtãƒ•ã‚¡ã‚¤ãƒ«ã§å­¦ç¿’

![Untitled Diagram (1).png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217395/9720302d-1947-a02c-0e3b-fc294a9da279.png)


## å‚è€ƒã«ã—ãŸã‚½ãƒ¼ã‚¹

ã“ã¡ã‚‰ã‚’å‚è€ƒã«ã•ã›ã¦é ‚ãã¾ã—ãŸğŸ‘‡  
â‘ [ã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆå…¥é–€ã€‘syamu_gameã¨ä¼šè©±ã—ã‚ˆã†](https://qiita.com/Umemiya/items/027f8bac0650c28590b5)  
â‘¡ [ã™ãã«ä½¿ãˆã‚‹ï¼æ¥­å‹™ã§å®Ÿè·µã§ãã‚‹ï¼Pythonã«ã‚ˆã‚‹ AIãƒ»æ©Ÿæ¢°å­¦ç¿’ãƒ»æ·±å±¤å­¦ç¿’ã‚¢ãƒ—ãƒªã®ã¤ãã‚Šæ–¹ (æ›¸ç±) - 4ç« ](https://github.com/kujirahand/book-mlearn-gyomu/tree/master/src/ch4)

## å…¨ä½“ã®æµã‚Œ

1. å®Ÿè¡Œç’°å¢ƒã‚’æ§‹ç¯‰(docker)
1. Jupyter Notebookã‚’é–‹ã
1. å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆï¼ˆmecabã¨word2vecï¼‰
1. ãƒãƒ«ã‚³ãƒ•é€£é–ç”¨ã®è¾æ›¸ã‚’ä½œæˆ
1. ãƒãƒ£ãƒƒãƒˆã™ã‚‹

## äº‹å‰æº–å‚™

- Docker Toolboxã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãŠã
- [ã“ã“](https://github.com/kujirahand/book-mlearn-gyomu/tree/master/src/)ã‹ã‚‰Dockerfileã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«ã‚³ãƒ”ãƒ¼ã•ã›ã¦é ‚ã

## 1. å®Ÿè¡Œç’°å¢ƒã‚’æ§‹ç¯‰(docker)

â‘  Docker Quickstart Terminalã‚’èµ·å‹•

<img width="623" alt="2019-04-25_00h54_59.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217395/fd0ef6e1-6a95-c3c5-a118-97b01e2a21eb.png">


â‘¡ cdã‚³ãƒãƒ³ãƒ‰ã§Dockerfileã‚’ã‚³ãƒ”ãƒ¼ã—ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•ã™ã‚‹

```sh
cd {Dockerfileã‚’ã‚³ãƒ”ãƒ¼ã—ãŸãƒ•ã‚©ãƒ«ãƒ€}
```

â‘¢ ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’ä½œæˆã™ã‚‹

```sh
docker build -t tanjun-chat-bot .
```

â‘£ ã‚³ãƒ³ãƒ†ãƒŠã‚’ä½œæˆãƒ»èµ·å‹•ã—ã‚³ãƒ³ãƒ†ãƒŠã«å…¥ã‚‹

```sh
docker run -it -p 8888:8888 -v /{Dockerfileã‚’ã‚³ãƒ”ãƒ¼ã—ãŸãƒ•ã‚©ãƒ«ãƒ€}:/{Dockerfileã‚’ã‚³ãƒ”ãƒ¼ã—ãŸãƒ•ã‚©ãƒ«ãƒ€å} --name tanjun-chat-bot tanjun-chat-bot
```

ä¾‹ï¼š`docker run -it -p 8888:8888 -v //c/Users/hogehoge/tanjun-chat-bot:/tanjun-chat-bot --name tanjun-chat-bot tanjun-chat-bot`

â€» åŒã˜å˜èªä½¿ã„ã™ã...  
`//c/Users/hogehoge/tanjun-chat-bot`â˜ ãƒã‚¦ãƒ³ãƒˆã™ã‚‹ãƒ›ã‚¹ãƒˆå´ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª  
`:/tanjun-chat-bot` â˜ ã‚³ãƒ³ãƒ†ãƒŠä¸Šã§ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª  
`--name tanjun-chat-bot`â˜ ã‚³ãƒ³ãƒ†ãƒŠå  
`tanjun-chat-bot` â˜ ã‚¤ãƒ¡ãƒ¼ã‚¸å  

â‘¤ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’Python2ã‹ã‚‰Python3ã«å¤‰æ›´

â€»å¤‰æ›´ã®å¿…è¦ã¯ãªã„ã§ã™ãŒã€jupyter notebookã‚’ä»‹ã•ãšã«`python`å‹•ã‹ã—ãŸã„ã¨ãã«ä¾¿åˆ©ãªã®ã§ä¸€å¿œå¤‰ãˆã¾ã™ã€‚

```sh
which python
```

ã¨æ‰“ã¤ã¨ã€`/usr/bin/python`ã¨è¡¨ç¤ºã•ã‚Œã€

```sh
ls -l /usr/bin/python
```

ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã‚’ç¢ºèªã™ã‚‹ã¨ã€`/usr/bin/python -> python2.7`ã¨python2ã«ç´ã¥ã„ã¦ã„ã‚‹ã®ã§ã€

```sh
ln -nfs /usr/bin/python3 /usr/bin/python
```

ã¨python3ã«ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã‚’å¤‰æ›´ã™ã‚‹ã€‚

## 2. Jupyter Notebookã‚’é–‹ã

dockerä¸Šã§èµ·å‹•ã—ãŸjupyter notebookã‚’ãƒ›ã‚¹ãƒˆOSã®ãƒ–ãƒ©ã‚¦ã‚¶ã§æ“ä½œã™ã‚‹ã€‚

â‘  dockerã‚’èµ·å‹•ã—ãŸã¨ãã«è¡¨ç¤ºã•ã‚ŒãŸIPã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’ç¢ºèªã™ã‚‹

<img width="445" alt="2019-06-12_21h42_40.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217395/09fabfc6-b28e-bb19-af6c-5541864991c0.png">


â‘¡ Jupyter Notebookã‚’èµ·å‹•ã™ã‚‹

```sh
jupyter notebook --no-browser --ip=0.0.0.0 --allow-root --NotebookApp.iopub_data_rate_limit=100000000
```

â‘¢ ãƒ–ãƒ©ã‚¦ã‚¶ã§Jupyter Notebookã‚’é–‹ã

ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã«è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹`http://(1a6611d8c398 or 127.0.0.1):8888/...`ã®`(1a6611d8c398 or 127.0.0.1)`éƒ¨åˆ†ã‚’ã€å…ˆã»ã©ç¢ºèªã—ãŸIPã‚¢ãƒ‰ãƒ¬ã‚¹ã«æ›¸ãæ›ãˆã€ãƒ–ãƒ©ã‚¦ã‚¶ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã€‚

â‘£ ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã«è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹tokenã‚’å…¥åŠ›ã—ã€ãƒ­ã‚°ã‚¤ãƒ³ã™ã‚‹

ãƒ–ãƒ©ã‚¦ã‚¶ã®å…¥åŠ›ãƒœãƒƒã‚¯ã‚¹ã«tokenã‚’è²¼ã‚Šä»˜ã‘ã€ãƒ­ã‚°ã‚¤ãƒ³ã™ã‚‹ã€‚`http://(ï½ or 127.0.0.1):8888/?token= {ã“ã“ã‚’ã‚³ãƒ”ãƒ¼ã™ã‚‹} `  

<img width="960" alt="2019-04-07_15h09_00.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217395/6aa4f11f-25eb-b779-bcd3-b16e3f33c1bc.png">


## 3. å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆï¼ˆmecabã¨word2vecï¼‰

â‘  ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã™ã‚‹

ç”»åƒã®ã‚ˆã†ã«ã€python3ã§å‹•ããƒãƒ¼ãƒˆã‚’ä½œæˆã™ã‚‹

<img width="920" alt="2019-04-08_21h12_16.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217395/dafa36b4-3308-c6cf-ee54-a084f036c0e3.png">


â‘¡ mecabã¨word2vecã‚’ä½¿ãˆã‚‹ã‚ˆã†ã«ã™ã‚‹

```py
!curl -kL https://bootstrap.pypa.io/get-pip.py | python3

!apt-get update -y
!apt-get upgrade -y
# ğŸ‘†ã«ã¤ã„ã¦ã€ååˆ†ãªç©ºãå®¹é‡ãŒãªã„ã¨ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹å ´åˆãŒã‚ã‚‹ã€‚ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ã‚°ã‚°ã£ãŸã‚‰è§£æ±ºæ³•ã§ã‚‹ã€‚

!apt-get -yV install swig-doc
!apt-get -yV install swig-examples
!apt-get -yV install swig2.0-doc
!apt-get -yV install swig2.0-
!apt-get -yV install swig2.0 
!apt-get -yV install swig

!apt-get install mecab libmecab-dev mecab-ipadic-utf8 -y

!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git /tmp/work/
# ğŸ‘†ã®cloneå…ˆã‚’ãƒ›ã‚¹ãƒˆOSã¨ã®å…±æœ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã—ã¦ã„ã‚‹ã¨ã‚¨ãƒ©ãƒ¼ã¨ãªã‚‹å ´åˆãŒã‚ã‚‹ã€‚
# ãã®ãŸã‚tmp/workã«ä¿å­˜ã—ã¦ã„ã‚‹

!mkdir /var/lib/mecab/dic/mecab-ipadic-neologd

!apt-get install file -y

!/tmp/work/bin/install-mecab-ipadic-neologd -n -p /var/lib/mecab/dic/mecab-ipadic-neologd -y

!pip install mecab-python3

import MeCab
import sys
import MeCab
m = MeCab.Tagger ("-d /var/lib/mecab/dic/mecab-ipadic-neologd")
print(m.parse ("ã™ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã®ã†ã¡"))
# ğŸ‘†mecabã®ç¨¼åƒç¢ºèª

!pip install gensim
# word2vecã®æº–å‚™
```

ä¸Šè¨˜ã®ã‚³ãƒãƒ³ãƒ‰ã‚’ã‚»ãƒ«ã«è²¼ã‚Šä»˜ã‘å®Ÿè¡Œã™ã‚‹ã€‚

<img width="960" alt="2019-06-12_22h36_21.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217395/628b8d09-8e3a-c718-49d7-fb83fb3640fc.png">

<img width="960" alt="2019-06-12_22h41_13.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217395/addcf554-24d3-c568-9089-44ffe4d7388c.png">


å®Œäº†ã«ã¯çµæ§‹æ™‚é–“ãŒã‹ã‹ã‚‹ã®ã§ã€æš‡ãªã¨ãã«ã‚„ã‚‹ã®ãŒãŠå‹§ã‚ã§ã™ã€‚

â‘¢ å­¦ç¿’ã•ã›ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’æ ¼ç´ã™ã‚‹

Dockerfileã¨åŒã˜éšå±¤ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«`data`ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã—ã€ã•ã‚‰ã«ãã®ä¸­ã«`learning`ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã€ãã®ä¸­ã«å­¦ç¿’ã•ã›ãŸã„`txtãƒ•ã‚¡ã‚¤ãƒ«`(utf-8ã‹shift_jis)ã‚’æ ¼ç´ã™ã‚‹ã€‚

â‘£ ã„ã‚ˆã„ã‚ˆæ–‡ç« ã‚’å­¦ç¿’ã•ã›ã‚‹

ã‚»ãƒ«ã‚’è¿½åŠ ã—ã€ä»¥ä¸‹ã‚’ã‚³ãƒ”ãƒšã€å®Ÿè¡Œã™ã‚‹

```py
#åˆ†ã‹ã¡æ›¸ãä½œæˆ
# å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã™ã‚‹ãŸã‚ã€‚åˆ†ã‹ã¡æ›¸ããƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
import MeCab
import os, glob

wakati_file = './data/wakati.txt'

def wakati(text):
        result = []
        #å„è¡Œã«åˆ†ã‘ã‚‹(windowsã®æ”¹è¡Œã‚³ãƒ¼ãƒ‰\r\nã‚’æƒ³å®š)
        lines = text.split('\r')
        for line in lines:
            line = line.replace('\n','')

            # å½¢æ…‹ç´ è§£æ
            tagger = MeCab.Tagger("-d /var/lib/mecab/dic/mecab-ipadic-neologd")

            for chunk in tagger.parse(line).splitlines()[:-1]:
                (surface, hinshi) = chunk.split('\t')
                if hinshi.startswith('åè©') or hinshi.startswith('å‹•è©') or hinshi.startswith('å½¢å®¹è©'):
    #           æ–‡ç« ã®ä¸€ç•ªæœ€åˆã«å‡ºã¦ãã‚‹åè©orå‹•è©orå½¢å®¹è©ã«ã¤ã„ã¦å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹
                    if '*' in hinshi.split(",")[6]:
    #                     åŸºæœ¬å½¢ãŒå­˜åœ¨ã—ãªã„å ´åˆã€è¡¨å±¤å½¢ã‚’è¿”ã™
                        result.append(surface)
                    else :
    #                     åŸºæœ¬å½¢ã‚’è¿”ã™
                        result.append(hinshi.split(",")[6])
                elif hinshi.startswith('æ„Ÿå‹•è©'):
                    result.append(surface)
        return result

def main(file):
    words = []
    file_dir = os.path.abspath(file)
    try:
        bindata = open(file_dir, 'rb').read()
        text = bindata.decode('shift_jis')
        words = wakati(text)
    except:
        try:
            text = bindata.decode('utf-8')
        except:
            try:
                text = bindata.decode('cp932')
            except Exception as e:
                print('error!',e)
                exit(0)

    with open(wakati_file, 'a', encoding='utf-8') as f:
        f.write(' '.join(words))

if __name__ == '__main__':
    if os.path.exists(wakati_file):
#         ã™ã§ã«ã‚ã‹ã¡æ›¸ããŒå­˜åœ¨ã™ã‚‹å ´åˆå‰Šé™¤
        os.remove(wakati_file)
        print('remove -> ' + wakati_file)
#   learningãƒ•ã‚©ãƒ«ãƒ€ç›´ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    files = glob.glob('./data/learning/*.txt')
    for file in files:
        if not 'wakati' in file:
            print(file)
            main(file)
```

ã“ã‚Œã§ã‚ã‹ã¡æ›¸ã(å˜èªã”ã¨ã«ãƒãƒ©ãƒãƒ©ã«ã—ãŸ)ãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã§ããŸã€‚

â‘£ å­¦ç¿’ç©ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ(æ©Ÿæ¢°å­¦ç¿’!)

ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é©å½“ã«è¨­å®šã—ã€word2vecã§ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã™ã‚‹

```py
# word2vecã§ãƒ¢ãƒ‡ãƒ«ä½œæˆ
from gensim.models import word2vec

wakati_file = './data/wakati.txt'
model_file = './data/daily_w2v_model.model'

if __name__ == '__main__':
    file_dir = wakati_file
    w2v_data = word2vec.LineSentence(file_dir)
    model = word2vec.Word2Vec(w2v_data, size=100, window=3, hs=1, min_count=1, sg=1)
    model.save(model_file)
```

`date`ãƒ•ã‚©ãƒ«ãƒ€ç›´ä¸‹ã«ã€`wakati.txt`ã¨`daily_w2v_model.model`ãŒã§ãã¦ã„ã¦ã€ã„ã„æ„Ÿã˜ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º(è»½ã™ããªã„)ã§ã‚ã‚Œã°å®Œäº†ã€‚

## 4. ãƒãƒ«ã‚³ãƒ•é€£é–ç”¨ã®è¾æ›¸ã‚’ä½œæˆ

â‘  ä¼šè©±ã®è¿”ç­”ã‚’ãƒãƒ«ã‚³ãƒ•é€£é–ã§è¿”ã™ãŸã‚ã®jsonãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ

â€»ã€Œãƒãƒ«ã‚³ãƒ•é€£é–ã£ã¦ï¼Ÿã€ã¨ã„ã†æ–¹ã¯ã‚°ã‚°ã£è¦‹ã¦ãã ã•ã„ã€‚è¦ã¯ã¤ã­ã«ã‚µã‚¤ã‚³ãƒ­ã‚’è»¢ãŒã—ã¦é¸æŠã™ã‚‹ã¨ã„ã†ã“ã¨ã§ã™ã€‚  
ã“ã“ã§ã„ã†ãƒãƒ«ã‚³ãƒ•é€£é–ã¯ã€ãƒœãƒƒãƒˆãŒã‚ã‚‹å˜èªã‚’ã—ã‚ƒã¹ã‚ã†ã¨ã—ãŸã¨ãã«ã€æ¬¡ã«ãªã‚“ã®å˜èªã‚’ã—ã‚ƒã¹ã‚‹ã®ã‹ã€ã‚µã‚¤ã‚³ãƒ­ã‚’æŒ¯ã£ã¦(ãƒ©ãƒ³ãƒ€ãƒ ã§é¸æŠ)å˜èªã‚’ç´¡ã„ã§ã„ãã¨ã„ã†ã‚¤ãƒ¡ãƒ¼ã‚¸ã§ã™ã€‚  

â‘  ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’æ–°ãŸãªã‚»ãƒ«ã«è²¼ã‚Šä»˜ã‘ã€å®Ÿè¡Œ

```py
# ãƒãƒ«ã‚³ãƒ•è¾æ›¸ã®ä½œæˆ
import MeCab
import json, os, glob

markov_file = "./data/markov_dict.json"
dic = {}

def wakati(text):
    wordlist = []
    lines = text.split('\r')
    for line in lines:
        line = line.replace('\n','')

        # å½¢æ…‹ç´ è§£æ
        tagger = MeCab.Tagger("-d /var/lib/mecab/dic/mecab-ipadic-neologd")

        for chunk in tagger.parse(line).splitlines()[:-1]:
            (surface, hinshi) = chunk.split('\t')

            if  hinshi not  in ["BOS/EOS"]:
                if hinshi.startswith('è¨˜å·'):
#                   è¨˜å·ã‚’è¾æ›¸ã«ç™»éŒ²ã—ãªã„
                    pass
                elif '*' in hinshi.split(",")[6]:
#                   åŸºæœ¬å½¢ãŒå­˜åœ¨ã—ãªã„å ´åˆã€è¡¨å±¤å½¢ã‚’è¿”ã™
                    wordlist.append([surface,hinshi])
                else :
#                   åŸºæœ¬å½¢ã‚’è¿”ã™
                    wordlist.append([hinshi.split(",")[6],hinshi])

    return wordlist

def regist_dic(wordlist):
    global dic
    w1 = ""
    w2 = ""

    # è¦ç´ ãŒ3æœªæº€ã®å ´åˆã¯ã€ä½•ã‚‚ã—ãªã„
    if len(wordlist) < 3 : return

    for w in wordlist :
        word = w[0]
        if word == "" or  word == "\r\n" or word == "\n" : continue
        # è¾æ›¸ã«å˜èªã‚’è¨­å®š
        if w1 and w2 :
            set_dic(dic,w1, w2, word)
        # æ–‡æœ«ã‚’è¡¨ã™èªã®ã®å ´åˆã€é€£é–ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹
        if word == "ã€‚" or word == "?" or  word == "ï¼Ÿ" :
            w1 = ""
            w2 = ""
            continue
        # æ¬¡ã®å‰å¾Œé–¢ä¿‚ã‚’ç™»éŒ²ã™ã‚‹ãŸã‚ã«ã€å˜èªã‚’ã‚¹ãƒ©ã‚¤ãƒ‰
        w1, w2 = w2, word

    # è¾æ›¸ã‚’ä¿å­˜
    json.dump(dic, open(markov_file,"w", encoding="utf-8"))

# è¾æ›¸ã«å˜èªã‚’è¨­å®š --- (*2)
def set_dic(dic, w1, w2, w3):
    # æ–°ã—ã„å˜èªã®å ´åˆã¯ã€æ–°ã—ã„è¾æ›¸ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
    if w1 not in dic : dic[w1] = {}
    if w2 not in dic[w1] : dic[w1][w2] = {}
    if w3 not in dic[w1][w2]: dic[w1][w2][w3] = 0
    # å˜èªã®å‡ºç¾æ•°ã‚’ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆã™ã‚‹
    dic[w1][w2][w3] += 1

if __name__ == '__main__':
        if os.path.exists(markov_file):
#         ã™ã§ã«ã‚ã‹ã¡æ›¸ããŒå­˜åœ¨ã™ã‚‹å ´åˆå‰Šé™¤
            os.remove(markov_file)
            print('remove -> ' + markov_file)
        files = glob.glob('./data/learning/*.txt')
        for file in files:
            if not 'wakati' in file:
                print(file)
                try:
                    bindata = open(file, 'rb').read()
                    text = bindata.decode('shift_jis')
                except:   
                    try:
                        text = bindata.decode('utf-8')
                    except:
                        try:
                            text = bindata.decode('cp932')
                        except Exception as e:
                            print('error!',e)  
                            exit(0)

                wordlist = wakati(text)
                regist_dic(wordlist)
```

`data`ãƒ•ã‚©ãƒ«ãƒ€ç›´ä¸‹ã«`markov_dict.json`ãŒã§ãã¦ã„ã‚Œã°OKã§ã™ã€‚

ä»¥ä¸Šã§ãƒãƒ«ã‚³ãƒ•é€£é–ã§è¿”ç­”ã™ã‚‹ãŸã‚ã®è¾æ›¸ãŒä½œæˆå®Œäº†ã§ã™ï¼

## 5. ãƒãƒ£ãƒƒãƒˆã™ã‚‹

ä½œæˆã—ãŸå­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã€ãƒãƒ«ã‚³ãƒ•é€£é–ç”¨è¾æ›¸ã‚’ç”¨ã„ã¦ã€ä¼šè©±ã—ã¦ã¿ã¾ã™ã€‚

```py
# chatã‚’é–‹å§‹
import MeCab
from gensim.models import word2vec
import json, random, os

markov_dict = "./data/markov_dict.json"
model_file = './data/daily_w2v_model.model'


def preprocessing(sentence):
    return sentence.rstrip()

def talk_markov(text): 
        # æ–‡ç« æ•´å½¢
        if text[-1] != "." or text[-1] != "?" or text[-1] != "ï¼Ÿ" or text[-1] != "!" or text[-1] != "ï¼":                     
            text = text[:-1]
            text += 'ã€‚'

        # å½¢æ…‹ç´ è§£æ
        tagger = MeCab.Tagger("-d /var/lib/mecab/dic/mecab-ipadic-neologd")

        sentence = preprocessing(text)

        for chunk in tagger.parse(sentence).splitlines()[:-1]:
            (surface, hinshi) = chunk.split('\t')
            if hinshi.startswith('åè©') or hinshi.startswith('å‹•è©') or hinshi.startswith('å½¢å®¹è©'):
#           æ–‡ç« ã®ä¸€ç•ªæœ€åˆã«å‡ºã¦ãã‚‹åè©orå‹•è©orå½¢å®¹è©ã«ã¤ã„ã¦å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹                
                if '*' in hinshi.split(",")[6]:
#                     åŸºæœ¬å½¢ãŒå­˜åœ¨ã—ãªã„å ´åˆã€è¡¨å±¤å½¢ã‚’è¿”ã™
                    return surface
                else :
#                     åŸºæœ¬å½¢ã‚’è¿”ã™
                    return hinshi.split(",")[6]
            elif hinshi.startswith('æ„Ÿå‹•è©'):
#           ã€€ã€€æ„Ÿå‹•è©ã®å ´åˆãã®ã¾ã¾è¿”ç­”
                return '@' + surface

        return '@'

def load_w2v(word):
    model = word2vec.Word2Vec.load(model_file)
    try:
        similar_words = model.wv.most_similar(positive=[word])
        return random.choice([w[0] for w in similar_words])
    except:
        return word

def word_choice(sel):
    keys = sel.keys()
    ran = random.choice(list(keys))
    return ran

def make_sentence(reply):
    if '@' in reply:
#       æ„Ÿå‹•è©ã®å ´åˆãã®ã¾ã¾è¿”ã™
        return word.replace('@','')

    markov_dic = json.load(open(markov_dict))
    if not reply == '':
        ret = []
        try:
            top = markov_dic[reply]
            word1 = word_choice(top)
            word2 = word_choice(top[word1])
            ret.append(reply)
            ret.append(word1)
            ret.append(word2)
        except Exception as e:
#             print('error:',e)
            return 'Sorry. There is no vocabulary.'

        while True:
            word3 = word_choice(markov_dic[word1][word2])
            ret.append(word3)
            if word3 == 'ã€‚':
                break
            if len(ret) >= 10:
                ret.append('ã€‚')
                break
            word1, word2 = word2, word3
        return ''.join(ret)
    else:
        return ''

if __name__ == '__main__':
    while True:
        input_message = input('you -> ')
        if input_message == 'bay' or input_message == '':
            print('bot -> ' + 'bay !')
            break
            exit(0)
        word = talk_markov(input_message)
        if not '@' == word:
            reply = load_w2v(word)
        else:
#           åè©ãƒ»å‹•è©ãƒ»å½¢å®¹è©ãƒ»æ„Ÿå‹•è©ä»¥å¤–ã®å ´åˆ
            reply = ''
        sentence = make_sentence(reply)
        print('bot -> ' + sentence)
```

`bay`ã¨æ‰“ã¤ã¨ãƒãƒ£ãƒƒãƒˆãŒçµ‚äº†ã—ã¾ã™ã€‚
å­¦ç¿’ã•ã›ãŸãƒ‡ãƒ¼ã‚¿ã«é–¢é€£ãƒ¯ãƒ¼ãƒ‰ãŒãªã„ã¨ã€`Sorry. There is no vocabulary.`ã¨è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚  

æŒ¨æ‹¶ãªã©ã€`æ„Ÿå‹•è©`ã¯å…¥åŠ›ã«å¯¾ã—ã¦ãã®ã¾ã¾è¿”ç­”ã—ã¾ã™ã€‚

<img width="960" alt="2019-06-13_00h28_57.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217395/f64bb2ba-4e97-acc1-bf51-8134596a79d6.png">

(ä¹…ã€…ã«ãƒ—ãƒ­ã‚°ãƒ©ãƒ å®Ÿè¡Œã—ã¦ã¿ãŸã‚‰ã‚¨ãƒ©ãƒ¼ã¯ã„ã¦ã¾ã™ã—ãŸ...ã€‚ãŸã ä¸€å¿œã¡ã‚ƒã‚“ã¨å‹•ã„ã¦ãã†ãªã®ã§ã€ã„ã£ãŸã‚“ã“ã®ã¾ã¾ã«ã—ã¦ã—ã¾ã„ã¾ã™ã€‚)

---

ä»¥ä¸Šã§ã™ã€‚

ã©ãªãŸã‹ã‚‚ãŠã£ã—ã‚ƒã£ã¦ã¾ã—ãŸãŒã€ã‹ã¿åˆã£ã¦ã‚‹ã‘ã©æŠ½è±¡çš„ãªä¼šè©±ã‚ˆã‚Šã€ã‹ã¿åˆã£ã¦ãªãã¦ã‚‚å›ºæœ‰åè©ãŒå‡ºã¦ãã‚‹ä¼šè©±ã®æ–¹ãŒé¢ç™½ã„ã‚ˆã†ã«æ€ã„ã¾ã™ã€‚

åˆå¿ƒè€…ã®æ–¹ã€å‚è€ƒã«ãªã‚‹éƒ¨åˆ†ãŒã‚ã‚Œã°ã†ã‚Œã—ã„ã§ã™ã€‚é–“é•ã£ãŸè¨˜è¼‰ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã®ã§ã€ç–‘ã£ã¦èª­ã‚“ã§ãã ã•ã„ã€‚  
æœ‰è­˜è€…ã®æ–¹ã€çªã£è¾¼ã¿ã©ã“ã‚ã‚³ãƒ¡ãƒ³ãƒˆãã ã•ã„ã€‚  
