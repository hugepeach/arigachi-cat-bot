{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹\n",
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -kL https://bootstrap.pypa.io/get-pip.py | python3\n",
    "\n",
    "!apt-get update -y\n",
    "!apt-get upgrade -y\n",
    "# ğŸ‘†ã«ã¤ã„ã¦ã€ååˆ†ãªç©ºãå®¹é‡ãŒãªã„ã¨ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹å ´åˆãŒã‚ã‚‹\n",
    "\n",
    "!apt-get -yV install swig-doc\n",
    "!apt-get -yV install swig-examples\n",
    "!apt-get -yV install swig2.0-doc\n",
    "!apt-get -yV install swig2.0-\n",
    "!apt-get -yV install swig2.0 \n",
    "!apt-get -yV install swig\n",
    "\n",
    "!apt-get install mecab libmecab-dev mecab-ipadic-utf8 -y\n",
    "\n",
    "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git /tmp/work/\n",
    "# ğŸ‘†ã®cloneå…ˆã‚’ãƒ›ã‚¹ãƒˆOSã¨ã®å…±æœ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã—ã¦ã„ã‚‹ã¨ã‚¨ãƒ©ãƒ¼ã¨ãªã‚‹å ´åˆãŒã‚ã‚‹ã€‚\n",
    "# ãã®ãŸã‚tmp/workã«ä¿å­˜ã—ã¦ã„ã‚‹\n",
    "\n",
    "!mkdir /var/lib/mecab/dic/mecab-ipadic-neologd\n",
    "\n",
    "!apt-get install file -y\n",
    "\n",
    "!/tmp/work/bin/install-mecab-ipadic-neologd -n -p /var/lib/mecab/dic/mecab-ipadic-neologd -y\n",
    "\n",
    "!pip install mecab-python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import sys\n",
    "import MeCab\n",
    "m = MeCab.Tagger (\"-d /var/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "print(m.parse (\"ã™ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã®ã†ã¡\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vecã®æº–å‚™\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#åˆ†ã‹ã¡æ›¸ãä½œæˆ\n",
    "# å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã™ã‚‹ãŸã‚ã€‚åˆ†ã‹ã¡æ›¸ããƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ\n",
    "import MeCab\n",
    "import os, glob\n",
    "\n",
    "wakati_file = './data/wakati.txt'\n",
    "\n",
    "def wakati(text): \n",
    "        result = []\n",
    "        #å„è¡Œã«åˆ†ã‘ã‚‹(windowsã®æ”¹è¡Œã‚³ãƒ¼ãƒ‰\\r\\nã‚’æƒ³å®š)\n",
    "        lines = text.split('\\r') \n",
    "        for line in lines:\n",
    "            line = line.replace('\\n','')\n",
    "\n",
    "            # å½¢æ…‹ç´ è§£æ\n",
    "            tagger = MeCab.Tagger(\"-d /var/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "\n",
    "            for chunk in tagger.parse(line).splitlines()[:-1]:\n",
    "                (surface, hinshi) = chunk.split('\\t')\n",
    "                if hinshi.startswith('åè©') or hinshi.startswith('å‹•è©') or hinshi.startswith('å½¢å®¹è©'):\n",
    "    #           æ–‡ç« ã®ä¸€ç•ªæœ€åˆã«å‡ºã¦ãã‚‹åè©orå‹•è©orå½¢å®¹è©ã«ã¤ã„ã¦å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹                \n",
    "                    if '*' in hinshi.split(\",\")[6]:\n",
    "    #                     åŸºæœ¬å½¢ãŒå­˜åœ¨ã—ãªã„å ´åˆã€è¡¨å±¤å½¢ã‚’è¿”ã™\n",
    "                        result.append(surface)\n",
    "                    else :\n",
    "    #                     åŸºæœ¬å½¢ã‚’è¿”ã™\n",
    "                        result.append(hinshi.split(\",\")[6]) \n",
    "                elif hinshi.startswith('æ„Ÿå‹•è©'):                    \n",
    "                    result.append(surface)         \n",
    "        return result        \n",
    "\n",
    "def main(file):\n",
    "    words = []\n",
    "    file_dir = os.path.abspath(file)\n",
    "    try:\n",
    "        bindata = open(file_dir, 'rb').read()     \n",
    "        text = bindata.decode('shift_jis')        \n",
    "        words = wakati(text)\n",
    "    except:        \n",
    "        try:\n",
    "            text = bindata.decode('utf-8')\n",
    "        except:\n",
    "            try:\n",
    "                text = bindata.decode('cp932')\n",
    "            except Exception as e:\n",
    "                print('error!',e)                \n",
    "                exit(0)\n",
    "\n",
    "    with open(wakati_file, 'a', encoding='utf-8') as f:\n",
    "        f.write(' '.join(words))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if os.path.exists(wakati_file):\n",
    "#         ã™ã§ã«ã‚ã‹ã¡æ›¸ããŒå­˜åœ¨ã™ã‚‹å ´åˆå‰Šé™¤\n",
    "        os.remove(wakati_file)\n",
    "        print('remove -> ' + wakati_file)\n",
    "#   learningãƒ•ã‚©ãƒ«ãƒ€ç›´ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "    files = glob.glob('./data/learning/*.txt')\n",
    "    for file in files:\n",
    "        if not 'wakati' in file:\n",
    "            print(file)\n",
    "            main(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vecã§ãƒ¢ãƒ‡ãƒ«ä½œæˆ\n",
    "from gensim.models import word2vec\n",
    "\n",
    "wakati_file = './data/wakati.txt'\n",
    "model_file = './data/daily_w2v_model.model'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    file_dir = wakati_file\n",
    "    w2v_data = word2vec.LineSentence(file_dir)\n",
    "    model = word2vec.Word2Vec(w2v_data, size=100, window=3, hs=1, min_count=1, sg=1)\n",
    "    model.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒãƒ«ã‚³ãƒ•è¾æ›¸ã®ä½œæˆ\n",
    "import MeCab\n",
    "import json, os, glob\n",
    "\n",
    "markov_file = \"./data/markov_dict.json\"\n",
    "dic = {}\n",
    "\n",
    "def wakati(text):\n",
    "    wordlist = []\n",
    "    lines = text.split('\\r') \n",
    "    for line in lines:\n",
    "        line = line.replace('\\n','')\n",
    "\n",
    "        # å½¢æ…‹ç´ è§£æ\n",
    "        tagger = MeCab.Tagger(\"-d /var/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "\n",
    "        for chunk in tagger.parse(line).splitlines()[:-1]:\n",
    "            (surface, hinshi) = chunk.split('\\t')\n",
    "\n",
    "            if  hinshi not  in [\"BOS/EOS\"]:\n",
    "                if hinshi.startswith('è¨˜å·'):\n",
    "#                   è¨˜å·ã‚’è¾æ›¸ã«ç™»éŒ²ã—ãªã„\n",
    "                    pass\n",
    "                elif '*' in hinshi.split(\",\")[6]:                    \n",
    "#                   åŸºæœ¬å½¢ãŒå­˜åœ¨ã—ãªã„å ´åˆã€è¡¨å±¤å½¢ã‚’è¿”ã™\n",
    "                    wordlist.append([surface,hinshi])\n",
    "                else :\n",
    "#                   åŸºæœ¬å½¢ã‚’è¿”ã™\n",
    "                    wordlist.append([hinshi.split(\",\")[6],hinshi])\n",
    "\n",
    "    return wordlist\n",
    "\n",
    "def regist_dic(wordlist):\n",
    "    global dic\n",
    "    w1 = \"\"\n",
    "    w2 = \"\"\n",
    "\n",
    "    # è¦ç´ ãŒ3æœªæº€ã®å ´åˆã¯ã€ä½•ã‚‚ã—ãªã„\n",
    "    if len(wordlist) < 3 : return\n",
    "\n",
    "    for w in wordlist :\n",
    "        word = w[0]\n",
    "        if word == \"\" or  word == \"\\r\\n\" or word == \"\\n\" : continue\n",
    "        # è¾æ›¸ã«å˜èªã‚’è¨­å®š\n",
    "        if w1 and w2 :\n",
    "            set_dic(dic,w1, w2, word)\n",
    "        # æ–‡æœ«ã‚’è¡¨ã™èªã®ã®å ´åˆã€é€£é–ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹\n",
    "        if word == \"ã€‚\" or word == \"?\" or  word == \"ï¼Ÿ\" :\n",
    "            w1 = \"\"\n",
    "            w2 = \"\"\n",
    "            continue\n",
    "        # æ¬¡ã®å‰å¾Œé–¢ä¿‚ã‚’ç™»éŒ²ã™ã‚‹ãŸã‚ã«ã€å˜èªã‚’ã‚¹ãƒ©ã‚¤ãƒ‰\n",
    "        w1, w2 = w2, word\n",
    "\n",
    "    # è¾æ›¸ã‚’ä¿å­˜\n",
    "    json.dump(dic, open(markov_file,\"w\", encoding=\"utf-8\"))\n",
    "\n",
    "# è¾æ›¸ã«å˜èªã‚’è¨­å®š --- (*2)\n",
    "def set_dic(dic, w1, w2, w3):\n",
    "    # æ–°ã—ã„å˜èªã®å ´åˆã¯ã€æ–°ã—ã„è¾æ›¸ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ\n",
    "    if w1 not in dic : dic[w1] = {}\n",
    "    if w2 not in dic[w1] : dic[w1][w2] = {}\n",
    "    if w3 not in dic[w1][w2]: dic[w1][w2][w3] = 0\n",
    "    # å˜èªã®å‡ºç¾æ•°ã‚’ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆã™ã‚‹\n",
    "    dic[w1][w2][w3] += 1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "        if os.path.exists(markov_file):\n",
    "#         ã™ã§ã«ã‚ã‹ã¡æ›¸ããŒå­˜åœ¨ã™ã‚‹å ´åˆå‰Šé™¤\n",
    "            os.remove(markov_file)\n",
    "            print('remove -> ' + markov_file)\n",
    "        files = glob.glob('./data/learning/*.txt')\n",
    "        for file in files:\n",
    "            if not 'wakati' in file:\n",
    "                print(file)\n",
    "                try:\n",
    "                    bindata = open(file, 'rb').read()\n",
    "                    text = bindata.decode('shift_jis')\n",
    "                except:   \n",
    "                    try:\n",
    "                        text = bindata.decode('utf-8')\n",
    "                    except:\n",
    "                        try:\n",
    "                            text = bindata.decode('cp932')\n",
    "                        except Exception as e:\n",
    "                            print('error!',e)  \n",
    "                            exit(0)\n",
    "\n",
    "                wordlist = wakati(text)\n",
    "                regist_dic(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatã‚’é–‹å§‹\n",
    "import MeCab\n",
    "from gensim.models import word2vec\n",
    "import json, random, os\n",
    "\n",
    "markov_dict = \"./data/markov_dict.json\"\n",
    "model_file = './data/daily_w2v_model.model'\n",
    "\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    return sentence.rstrip()\n",
    "\n",
    "def talk_markov(text): \n",
    "        # æ–‡ç« æ•´å½¢\n",
    "        if text[-1] != \".\" or text[-1] != \"?\" or text[-1] != \"ï¼Ÿ\" or text[-1] != \"!\" or text[-1] != \"ï¼\":                     \n",
    "            text = text[:-1]\n",
    "            text += 'ã€‚'\n",
    "\n",
    "        # å½¢æ…‹ç´ è§£æ\n",
    "        tagger = MeCab.Tagger(\"-d /var/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "\n",
    "        sentence = preprocessing(text)\n",
    "\n",
    "        for chunk in tagger.parse(sentence).splitlines()[:-1]:\n",
    "            (surface, hinshi) = chunk.split('\\t')\n",
    "            if hinshi.startswith('åè©') or hinshi.startswith('å‹•è©') or hinshi.startswith('å½¢å®¹è©'):\n",
    "#           æ–‡ç« ã®ä¸€ç•ªæœ€åˆã«å‡ºã¦ãã‚‹åè©orå‹•è©orå½¢å®¹è©ã«ã¤ã„ã¦å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹                \n",
    "                if '*' in hinshi.split(\",\")[6]:\n",
    "#                     åŸºæœ¬å½¢ãŒå­˜åœ¨ã—ãªã„å ´åˆã€è¡¨å±¤å½¢ã‚’è¿”ã™\n",
    "                    return surface\n",
    "                else :\n",
    "#                     åŸºæœ¬å½¢ã‚’è¿”ã™\n",
    "                    return hinshi.split(\",\")[6]\n",
    "            elif hinshi.startswith('æ„Ÿå‹•è©'):\n",
    "#           ã€€ã€€æ„Ÿå‹•è©ã®å ´åˆãã®ã¾ã¾è¿”ç­”\n",
    "                return '@' + surface\n",
    "\n",
    "        return '@'\n",
    "\n",
    "def load_w2v(word):\n",
    "    model = word2vec.Word2Vec.load(model_file)\n",
    "    try:\n",
    "        similar_words = model.wv.most_similar(positive=[word])\n",
    "        return random.choice([w[0] for w in similar_words])\n",
    "    except:\n",
    "        return word\n",
    "\n",
    "def word_choice(sel):\n",
    "    keys = sel.keys()\n",
    "    ran = random.choice(list(keys))\n",
    "    return ran\n",
    "\n",
    "def make_sentence(reply):\n",
    "    if '@' in reply:\n",
    "#       æ„Ÿå‹•è©ã®å ´åˆãã®ã¾ã¾è¿”ã™\n",
    "        return word.replace('@','')\n",
    "\n",
    "    markov_dic = json.load(open(markov_dict))\n",
    "    if not reply == '':\n",
    "        ret = []\n",
    "        try:\n",
    "            top = markov_dic[reply]\n",
    "            word1 = word_choice(top)\n",
    "            word2 = word_choice(top[word1])\n",
    "            ret.append(reply)\n",
    "            ret.append(word1)\n",
    "            ret.append(word2)\n",
    "        except Exception as e:\n",
    "#             print('error:',e)\n",
    "            return 'Sorry. There is no vocabulary.'\n",
    "\n",
    "        while True:\n",
    "            word3 = word_choice(markov_dic[word1][word2])\n",
    "            ret.append(word3)\n",
    "            if word3 == 'ã€‚':\n",
    "                break\n",
    "            if len(ret) >= 10:\n",
    "                ret.append('ã€‚')\n",
    "                break\n",
    "            word1, word2 = word2, word3\n",
    "        return ''.join(ret)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    while True:\n",
    "        input_message = input('you -> ')\n",
    "        if input_message == 'bay' or input_message == '':\n",
    "            print('bot -> ' + 'bay !')\n",
    "            break\n",
    "            exit(0)\n",
    "        word = talk_markov(input_message)\n",
    "        if not '@' == word:\n",
    "            reply = load_w2v(word)\n",
    "        else:\n",
    "#           åè©ãƒ»å‹•è©ãƒ»å½¢å®¹è©ãƒ»æ„Ÿå‹•è©ä»¥å¤–ã®å ´åˆ\n",
    "            reply = ''\n",
    "        sentence = make_sentence(reply)\n",
    "        print('bot -> ' + sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
