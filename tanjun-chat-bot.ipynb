{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エラーが出る\n",
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -kL https://bootstrap.pypa.io/get-pip.py | python3\n",
    "\n",
    "!apt-get update -y\n",
    "!apt-get upgrade -y\n",
    "# 👆について、十分な空き容量がないとエラーになる場合がある\n",
    "\n",
    "!apt-get -yV install swig-doc\n",
    "!apt-get -yV install swig-examples\n",
    "!apt-get -yV install swig2.0-doc\n",
    "!apt-get -yV install swig2.0-\n",
    "!apt-get -yV install swig2.0 \n",
    "!apt-get -yV install swig\n",
    "\n",
    "!apt-get install mecab libmecab-dev mecab-ipadic-utf8 -y\n",
    "\n",
    "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git /tmp/work/\n",
    "# 👆のclone先をホストOSとの共有ディレクトリにしているとエラーとなる場合がある。\n",
    "# そのためtmp/workに保存している\n",
    "\n",
    "!mkdir /var/lib/mecab/dic/mecab-ipadic-neologd\n",
    "\n",
    "!apt-get install file -y\n",
    "\n",
    "!/tmp/work/bin/install-mecab-ipadic-neologd -n -p /var/lib/mecab/dic/mecab-ipadic-neologd -y\n",
    "\n",
    "!pip install mecab-python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import sys\n",
    "import MeCab\n",
    "m = MeCab.Tagger (\"-d /var/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "print(m.parse (\"すもももももももものうち\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vecの準備\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分かち書き作成\n",
    "# 学習済みモデルを作成するため。分かち書きファイルを作成\n",
    "import MeCab\n",
    "import os, glob\n",
    "\n",
    "wakati_file = './data/wakati.txt'\n",
    "\n",
    "def wakati(text): \n",
    "        result = []\n",
    "        #各行に分ける(windowsの改行コード\\r\\nを想定)\n",
    "        lines = text.split('\\r') \n",
    "        for line in lines:\n",
    "            line = line.replace('\\n','')\n",
    "\n",
    "            # 形態素解析\n",
    "            tagger = MeCab.Tagger(\"-d /var/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "\n",
    "            for chunk in tagger.parse(line).splitlines()[:-1]:\n",
    "                (surface, hinshi) = chunk.split('\\t')\n",
    "                if hinshi.startswith('名詞') or hinshi.startswith('動詞') or hinshi.startswith('形容詞'):\n",
    "    #           文章の一番最初に出てくる名詞or動詞or形容詞について処理を実行する                \n",
    "                    if '*' in hinshi.split(\",\")[6]:\n",
    "    #                     基本形が存在しない場合、表層形を返す\n",
    "                        result.append(surface)\n",
    "                    else :\n",
    "    #                     基本形を返す\n",
    "                        result.append(hinshi.split(\",\")[6]) \n",
    "                elif hinshi.startswith('感動詞'):                    \n",
    "                    result.append(surface)         \n",
    "        return result        \n",
    "\n",
    "def main(file):\n",
    "    words = []\n",
    "    file_dir = os.path.abspath(file)\n",
    "    try:\n",
    "        bindata = open(file_dir, 'rb').read()     \n",
    "        text = bindata.decode('shift_jis')        \n",
    "        words = wakati(text)\n",
    "    except:        \n",
    "        try:\n",
    "            text = bindata.decode('utf-8')\n",
    "        except:\n",
    "            try:\n",
    "                text = bindata.decode('cp932')\n",
    "            except Exception as e:\n",
    "                print('error!',e)                \n",
    "                exit(0)\n",
    "\n",
    "    with open(wakati_file, 'a', encoding='utf-8') as f:\n",
    "        f.write(' '.join(words))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if os.path.exists(wakati_file):\n",
    "#         すでにわかち書きが存在する場合削除\n",
    "        os.remove(wakati_file)\n",
    "        print('remove -> ' + wakati_file)\n",
    "#   learningフォルダ直下のテキストファイルを読み込む\n",
    "    files = glob.glob('./data/learning/*.txt')\n",
    "    for file in files:\n",
    "        if not 'wakati' in file:\n",
    "            print(file)\n",
    "            main(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vecでモデル作成\n",
    "from gensim.models import word2vec\n",
    "\n",
    "wakati_file = './data/wakati.txt'\n",
    "model_file = './data/daily_w2v_model.model'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    file_dir = wakati_file\n",
    "    w2v_data = word2vec.LineSentence(file_dir)\n",
    "    model = word2vec.Word2Vec(w2v_data, size=100, window=3, hs=1, min_count=1, sg=1)\n",
    "    model.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# マルコフ辞書の作成\n",
    "import MeCab\n",
    "import json, os, glob\n",
    "\n",
    "markov_file = \"./data/markov_dict.json\"\n",
    "dic = {}\n",
    "\n",
    "def wakati(text):\n",
    "    wordlist = []\n",
    "    lines = text.split('\\r') \n",
    "    for line in lines:\n",
    "        line = line.replace('\\n','')\n",
    "\n",
    "        # 形態素解析\n",
    "        tagger = MeCab.Tagger(\"-d /var/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "\n",
    "        for chunk in tagger.parse(line).splitlines()[:-1]:\n",
    "            (surface, hinshi) = chunk.split('\\t')\n",
    "\n",
    "            if  hinshi not  in [\"BOS/EOS\"]:\n",
    "                if hinshi.startswith('記号'):\n",
    "#                   記号を辞書に登録しない\n",
    "                    pass\n",
    "                elif '*' in hinshi.split(\",\")[6]:                    \n",
    "#                   基本形が存在しない場合、表層形を返す\n",
    "                    wordlist.append([surface,hinshi])\n",
    "                else :\n",
    "#                   基本形を返す\n",
    "                    wordlist.append([hinshi.split(\",\")[6],hinshi])\n",
    "\n",
    "    return wordlist\n",
    "\n",
    "def regist_dic(wordlist):\n",
    "    global dic\n",
    "    w1 = \"\"\n",
    "    w2 = \"\"\n",
    "\n",
    "    # 要素が3未満の場合は、何もしない\n",
    "    if len(wordlist) < 3 : return\n",
    "\n",
    "    for w in wordlist :\n",
    "        word = w[0]\n",
    "        if word == \"\" or  word == \"\\r\\n\" or word == \"\\n\" : continue\n",
    "        # 辞書に単語を設定\n",
    "        if w1 and w2 :\n",
    "            set_dic(dic,w1, w2, word)\n",
    "        # 文末を表す語のの場合、連鎖をクリアする\n",
    "        if word == \"。\" or word == \"?\" or  word == \"？\" :\n",
    "            w1 = \"\"\n",
    "            w2 = \"\"\n",
    "            continue\n",
    "        # 次の前後関係を登録するために、単語をスライド\n",
    "        w1, w2 = w2, word\n",
    "\n",
    "    # 辞書を保存\n",
    "    json.dump(dic, open(markov_file,\"w\", encoding=\"utf-8\"))\n",
    "\n",
    "# 辞書に単語を設定 --- (*2)\n",
    "def set_dic(dic, w1, w2, w3):\n",
    "    # 新しい単語の場合は、新しい辞書オブジェクトを作成\n",
    "    if w1 not in dic : dic[w1] = {}\n",
    "    if w2 not in dic[w1] : dic[w1][w2] = {}\n",
    "    if w3 not in dic[w1][w2]: dic[w1][w2][w3] = 0\n",
    "    # 単語の出現数をインクリメントする\n",
    "    dic[w1][w2][w3] += 1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "        if os.path.exists(markov_file):\n",
    "#         すでにわかち書きが存在する場合削除\n",
    "            os.remove(markov_file)\n",
    "            print('remove -> ' + markov_file)\n",
    "        files = glob.glob('./data/learning/*.txt')\n",
    "        for file in files:\n",
    "            if not 'wakati' in file:\n",
    "                print(file)\n",
    "                try:\n",
    "                    bindata = open(file, 'rb').read()\n",
    "                    text = bindata.decode('shift_jis')\n",
    "                except:   \n",
    "                    try:\n",
    "                        text = bindata.decode('utf-8')\n",
    "                    except:\n",
    "                        try:\n",
    "                            text = bindata.decode('cp932')\n",
    "                        except Exception as e:\n",
    "                            print('error!',e)  \n",
    "                            exit(0)\n",
    "\n",
    "                wordlist = wakati(text)\n",
    "                regist_dic(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatを開始\n",
    "import MeCab\n",
    "from gensim.models import word2vec\n",
    "import json, random, os\n",
    "\n",
    "markov_dict = \"./data/markov_dict.json\"\n",
    "model_file = './data/daily_w2v_model.model'\n",
    "\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    return sentence.rstrip()\n",
    "\n",
    "def talk_markov(text): \n",
    "        # 文章整形\n",
    "        if text[-1] != \".\" or text[-1] != \"?\" or text[-1] != \"？\" or text[-1] != \"!\" or text[-1] != \"！\":                     \n",
    "            text = text[:-1]\n",
    "            text += '。'\n",
    "\n",
    "        # 形態素解析\n",
    "        tagger = MeCab.Tagger(\"-d /var/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "\n",
    "        sentence = preprocessing(text)\n",
    "\n",
    "        for chunk in tagger.parse(sentence).splitlines()[:-1]:\n",
    "            (surface, hinshi) = chunk.split('\\t')\n",
    "            if hinshi.startswith('名詞') or hinshi.startswith('動詞') or hinshi.startswith('形容詞'):\n",
    "#           文章の一番最初に出てくる名詞or動詞or形容詞について処理を実行する                \n",
    "                if '*' in hinshi.split(\",\")[6]:\n",
    "#                     基本形が存在しない場合、表層形を返す\n",
    "                    return surface\n",
    "                else :\n",
    "#                     基本形を返す\n",
    "                    return hinshi.split(\",\")[6]\n",
    "            elif hinshi.startswith('感動詞'):\n",
    "#           　　感動詞の場合そのまま返答\n",
    "                return '@' + surface\n",
    "\n",
    "        return '@'\n",
    "\n",
    "def load_w2v(word):\n",
    "    model = word2vec.Word2Vec.load(model_file)\n",
    "    try:\n",
    "        similar_words = model.wv.most_similar(positive=[word])\n",
    "        return random.choice([w[0] for w in similar_words])\n",
    "    except:\n",
    "        return word\n",
    "\n",
    "def word_choice(sel):\n",
    "    keys = sel.keys()\n",
    "    ran = random.choice(list(keys))\n",
    "    return ran\n",
    "\n",
    "def make_sentence(reply):\n",
    "    if '@' in reply:\n",
    "#       感動詞の場合そのまま返す\n",
    "        return word.replace('@','')\n",
    "\n",
    "    markov_dic = json.load(open(markov_dict))\n",
    "    if not reply == '':\n",
    "        ret = []\n",
    "        try:\n",
    "            top = markov_dic[reply]\n",
    "            word1 = word_choice(top)\n",
    "            word2 = word_choice(top[word1])\n",
    "            ret.append(reply)\n",
    "            ret.append(word1)\n",
    "            ret.append(word2)\n",
    "        except Exception as e:\n",
    "#             print('error:',e)\n",
    "            return 'Sorry. There is no vocabulary.'\n",
    "\n",
    "        while True:\n",
    "            word3 = word_choice(markov_dic[word1][word2])\n",
    "            ret.append(word3)\n",
    "            if word3 == '。':\n",
    "                break\n",
    "            if len(ret) >= 10:\n",
    "                ret.append('。')\n",
    "                break\n",
    "            word1, word2 = word2, word3\n",
    "        return ''.join(ret)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    while True:\n",
    "        input_message = input('you -> ')\n",
    "        if input_message == 'bay' or input_message == '':\n",
    "            print('bot -> ' + 'bay !')\n",
    "            break\n",
    "            exit(0)\n",
    "        word = talk_markov(input_message)\n",
    "        if not '@' == word:\n",
    "            reply = load_w2v(word)\n",
    "        else:\n",
    "#           名詞・動詞・形容詞・感動詞以外の場合\n",
    "            reply = ''\n",
    "        sentence = make_sentence(reply)\n",
    "        print('bot -> ' + sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
